{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Cluster Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mongo-Spark Connector Configuration to Connect to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T21:05:56.108419Z",
     "start_time": "2019-03-12T21:05:56.065038Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars.packages': 'org.mongodb.spark:mongo-spark-connector_2.11:2.4.0'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>None</td><td>pyspark</td><td>starting</td><td></td><td></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"conf\": {\"spark.jars.packages\": \"org.mongodb.spark:mongo-spark-connector_2.11:2.4.0\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T21:20:22.497438Z",
     "start_time": "2019-03-12T21:20:22.479790Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, when, count, col, lag, avg, lit\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Data from MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bikeshare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T21:07:20.586030Z",
     "start_time": "2019-03-12T21:06:37.126882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- starttime: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "mongodb_path = \"mongodb://172.31.38.7/msds697.bikeshare\"\n",
    "\n",
    "spark_bike = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"group25\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", mongodb_path)\\\n",
    "    .config(\"spark.driver.memory\", \"24g\")\\\n",
    "    .config(\"spark.yarn.appMasterEnv.PYSPARK_PYTHON\", \"python36\")\\\n",
    "    .config(\"spark.executorEnv.PYSPARK_PYTHON\", \"python36\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "bikeshare = spark_bike.read.format(\"com.mongodb.spark.sql.DefaultSource\").load().drop('_id')  # Load Data\n",
    "bikeshare = bikeshare.select('starttime').filter(\"starttime != 'starttime'\").cache()  # Keep only starttime column and filter out erroneous header line\n",
    "\n",
    "bikeshare.printSchema()\n",
    "# bikeshare.count()  # 47758652"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AQI Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T21:07:22.843426Z",
     "start_time": "2019-03-12T21:07:20.587929Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- aqi: integer (nullable = true)\n",
      " |-- siteID: integer (nullable = true)\n",
      " |-- yyyy/mm/dd: string (nullable = true)\n",
      "\n",
      "12859"
     ]
    }
   ],
   "source": [
    "mongodb_path = \"mongodb://172.31.38.7/aqi.aqi\"\n",
    "\n",
    "spark_aqi = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"group25\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", mongodb_path)\\\n",
    "    .config(\"spark.driver.memory\", \"24g\")\\\n",
    "    .config(\"spark.yarn.appMasterEnv.PYSPARK_PYTHON\", \"python36\")\\\n",
    "    .config(\"spark.executorEnv.PYSPARK_PYTHON\", \"python36\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "aqi = spark_aqi.read.format(\"com.mongodb.spark.sql.DefaultSource\").load().drop('_id').cache()\n",
    "aqi = aqi.select(['aqi', 'siteID', 'yyyy/mm/dd'])  # Keep only starttime column\n",
    "\n",
    "aqi.printSchema()\n",
    "aqi_total_count = aqi.count()\n",
    "print(aqi_total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T21:07:22.865691Z",
     "start_time": "2019-03-12T21:07:22.845288Z"
    }
   },
   "outputs": [],
   "source": [
    "def formatDateRide(mydate):\n",
    "    \"\"\"\n",
    "    Re-formats date to match bike date \n",
    "    \"\"\"\n",
    "    mydate = mydate.split(' ')[0]\n",
    "    try:\n",
    "        objDate = datetime.strptime(mydate, '%m/%d/%Y')\n",
    "        return datetime.strftime(objDate,'%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        return mydate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T21:07:23.096913Z",
     "start_time": "2019-03-12T21:07:22.867484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- count: long (nullable = false)"
     ]
    }
   ],
   "source": [
    "# Extract only date from timestamp\n",
    "date_udf = udf(lambda date: formatDateRide(date))\n",
    "daily_ridership = bikeshare.select(\"starttime\").withColumn(\"date\", date_udf(\"starttime\")).groupBy(\"date\").count()\n",
    "daily_ridership.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily AQI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T21:07:23.120552Z",
     "start_time": "2019-03-12T21:07:23.098766Z"
    }
   },
   "outputs": [],
   "source": [
    "def formatDate(mydate):\n",
    "    \"\"\"\n",
    "    Re-formats date to match bike date \n",
    "    \"\"\"\n",
    "    objDate = datetime.strptime(mydate, '%Y/%m/%d')\n",
    "    return datetime.strftime(objDate, '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T21:07:23.351731Z",
     "start_time": "2019-03-12T21:07:23.122319Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- max(AQI): integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "date_udf = udf(lambda date: formatDate(date))\n",
    "\n",
    "# Changing format of date column\n",
    "aqi = aqi.withColumn(\"date\", date_udf(\"yyyy/mm/dd\")).drop(\"siteID\", \"yyyy/mm/dd\")\n",
    "\n",
    "# Maintaining only the two stations near NYC\n",
    "NYC_sites = aqi.filter(\"SiteID == 360610135 or SiteID == 340171002\")\n",
    "\n",
    "# Group by Date\n",
    "daily_aqi = NYC_sites.groupBy(\"date\").max(\"AQI\")\n",
    "\n",
    "daily_aqi.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining AQI & Bike Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T21:07:23.583368Z",
     "start_time": "2019-03-12T21:07:23.353639Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      " |-- max(AQI): integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "bike_aqi_joined = daily_ridership.join(daily_aqi, 'date', 'inner')\n",
    "bike_aqi_joined.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous Day's AQI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T21:07:23.812229Z",
     "start_time": "2019-03-12T21:07:23.585302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      " |-- max(AQI): integer (nullable = true)\n",
      " |-- pre_AQI: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "# Adding AQI lagged at 1 day\n",
    "bike_aqi_joined = bike_aqi_joined.select('date', 'count', 'max(AQI)',\\\n",
    "                       lag('max(AQI)', 1).over(Window.orderBy('date'))\\\n",
    "                      .alias('pre_AQI'))\n",
    "\n",
    "bike_aqi_joined.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Season Indicator Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T21:07:23.835805Z",
     "start_time": "2019-03-12T21:07:23.814219Z"
    }
   },
   "outputs": [],
   "source": [
    "def getMonth(inval):\n",
    "    try:\n",
    "        return int(datetime.strptime(inval, \"%Y-%m-%d\").month)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def getYear(inval):\n",
    "    try:\n",
    "        return int(datetime.strptime(inval, \"%Y-%m-%d\").year)\n",
    "    except ValueError:\n",
    "        return None\n",
    "def getSeason(month):\n",
    "    season_dict = {'winter' : [1, 2, 12],\n",
    "                   'summer' : [6, 7, 8],\n",
    "                   'spring' : [3, 4, 5],\n",
    "                   'autumn' : [9, 10, 11]}\n",
    "    for season in season_dict:\n",
    "        if month in season_dict.get(season):\n",
    "            return season "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T21:07:24.062772Z",
     "start_time": "2019-03-12T21:07:23.837558Z"
    }
   },
   "outputs": [],
   "source": [
    "year_udf = udf(lambda date: getYear(date))\n",
    "month_udf = udf(lambda date: getMonth(date))\n",
    "season_udf = udf(lambda month: getSeason(int(month)))\n",
    "\n",
    "bike_aqi_joined = bike_aqi_joined.withColumn(\"month\", month_udf(bike_aqi_joined.date))\\\n",
    "                .withColumn(\"year\", year_udf(bike_aqi_joined.date))\n",
    "bike_aqi_joined = bike_aqi_joined.withColumn(\"season\", season_udf(bike_aqi_joined.month))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Out All Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T21:07:24.289295Z",
     "start_time": "2019-03-12T21:07:24.064583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- count: long (nullable = false)\n",
      " |-- max(AQI): integer (nullable = true)\n",
      " |-- pre_AQI: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- season: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "bike_ml = bike_aqi_joined.filter(bike_aqi_joined.pre_AQI.isNotNull()).drop('date')\n",
    "bike_ml.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Categorical Values to One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T21:15:01.308752Z",
     "start_time": "2019-03-12T21:07:24.291381Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 45798)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 696, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 266, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 254, in authenticate_and_accum_updates\n",
      "    received_token = self.rfile.read(len(auth_token))\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "----------------------------------------"
     ]
    }
   ],
   "source": [
    "# Convert to Numeric type first\n",
    "for _ in range(2):\n",
    "    si = StringIndexer(inputCol=\"season\", outputCol=\"season-num\")\n",
    "    sm = si.fit(bike_ml)\n",
    "    bike_num = sm.transform(bike_ml).drop(\"season\")\n",
    "    bike_num = bike_num.withColumnRenamed(\"season-num\", \"season\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T21:15:01.533059Z",
     "start_time": "2019-03-12T21:15:01.310688Z"
    }
   },
   "outputs": [],
   "source": [
    "# Then One-Hot encode numerics\n",
    "onehotenc = OneHotEncoder(inputCol=\"season\", outputCol=\"season-onehot\", dropLast=True)\n",
    "bike_ohe = onehotenc.transform(bike_num).drop(\"season\")\n",
    "bike_ohe = bike_ohe.withColumnRenamed(\"season-onehot\", \"season\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T21:15:01.552827Z",
     "start_time": "2019-03-12T21:15:01.535003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- count: long (nullable = false)\n",
      " |-- max(AQI): integer (nullable = true)\n",
      " |-- pre_AQI: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- season: vector (nullable = true)"
     ]
    }
   ],
   "source": [
    "bike_ohe.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Training (Pre-2018) and Test Sets (2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T21:15:01.776355Z",
     "start_time": "2019-03-12T21:15:01.554795Z"
    }
   },
   "outputs": [],
   "source": [
    "test = bike_ohe.filter(\"year == 2018 and month >= 1\")\n",
    "training = bike_ohe.subtract(test)\n",
    "\n",
    "test = test.drop('month', 'year')\n",
    "training = training.drop('month', 'year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Feature Vector and Label Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T21:16:47.400580Z",
     "start_time": "2019-03-12T21:15:01.778276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[23849.0,42.0,0.0...|   20|\n",
      "|[37802.0,25.0,0.0...|   22|\n",
      "|[14588.0,32.0,0.0...|   42|\n",
      "|[40637.0,25.0,1.0...|   19|\n",
      "|[41490.0,40.0,0.0...|   10|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(5,[0,1],[5500.0,...|   28|\n",
      "|(5,[0,1],[18818.0...|   30|\n",
      "|(5,[0,1],[24299.0...|   51|\n",
      "|(5,[0,1],[1922.0,...|   41|\n",
      "|(5,[0,1],[4972.0,...|   22|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "input_cols = [\"count\", \"pre_AQI\", \"season\"]\n",
    "va = VectorAssembler(outputCol=\"features\", inputCols=input_cols)\n",
    "\n",
    "bike_labeled_train = va.transform(training).select(\"features\", \"max(AQI)\").withColumnRenamed(\"max(AQI)\", \"label\")\n",
    "bike_labeled_test = va.transform(test).select(\"features\", \"max(AQI)\").withColumnRenamed(\"max(AQI)\", \"label\")\n",
    "\n",
    "bike_labeled_train.show(5)\n",
    "bike_labeled_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T21:16:48.132084Z",
     "start_time": "2019-03-12T21:16:47.402321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[features: vector, label: int]"
     ]
    }
   ],
   "source": [
    "# Persist on Memory for Faster Access\n",
    "bike_labeled_train.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "bike_labeled_test.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Validation Split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "80/20 split from training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T21:16:48.360753Z",
     "start_time": "2019-03-12T21:16:48.134022Z"
    }
   },
   "outputs": [],
   "source": [
    "train, valid = bike_labeled_train.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline modeling would be to predict the AQI for the day to be to be the mean AQI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T21:19:24.168746Z",
     "start_time": "2019-03-12T21:16:48.362584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average AQI Over Train-Split Dataset: 27.650969529085874\n",
      "Baseline Validation RMSE = 9.3701\n",
      "Baseline Test RMSE = 16.0113"
     ]
    }
   ],
   "source": [
    "train_avg_aqi = training.select('max(AQI)').agg({'max(AQI)': 'avg'}).collect()[0][0]  # best constant prediction\n",
    "print(f\"Average AQI Over Train-Split Dataset: {train_avg_aqi}\")\n",
    "\n",
    "baseline_val = valid\n",
    "baseline_val = baseline_val.withColumn('prediction', lit(train_avg_aqi))\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "print(f\"Baseline Validation RMSE = {round(evaluator.evaluate(baseline_val), 4)}\")\n",
    "\n",
    "baseline_test = bike_labeled_test\n",
    "baseline_test = baseline_test.withColumn('prediction', lit(train_avg_aqi))\n",
    "\n",
    "print(f\"Baseline Test RMSE = {round(evaluator.evaluate(baseline_test), 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T21:19:24.186351Z",
     "start_time": "2019-03-12T21:19:24.170530Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_exec_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T22:11:05.954749Z",
     "start_time": "2019-03-12T21:20:27.670028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossValidatorModel_ea1bcdeed52b\n",
      "CrossValidatorModel_346df6e21955\n",
      "CrossValidatorModel_0e96f4e31ded\n",
      "CrossValidatorModel_613c378d0361\n",
      "CrossValidatorModel_9a602b674da6\n",
      "CrossValidatorModel_d6341372ab15\n",
      "CrossValidatorModel_881a298d8322\n",
      "CrossValidatorModel_3b2e7d8bf45c\n",
      "CrossValidatorModel_45b65254eb4a\n",
      "CrossValidatorModel_e5c57f8f8de5\n",
      "CrossValidatorModel_7182e1c0c93f\n",
      "CrossValidatorModel_b864f40424e5\n",
      "CrossValidatorModel_64d460906241\n",
      "CrossValidatorModel_87a8c29e9c61\n",
      "CrossValidatorModel_1ac20747bffd\n",
      "CrossValidatorModel_5271fa4b44ee\n",
      "CrossValidatorModel_5f2989324a27\n",
      "CrossValidatorModel_dc5c5a8b2592\n",
      "CrossValidatorModel_0118c83b83aa\n",
      "CrossValidatorModel_f9c1d2a1064f\n",
      "CrossValidatorModel_641e321a504d\n",
      "CrossValidatorModel_34cbd5c66735\n",
      "CrossValidatorModel_4e0e22026eb1\n",
      "CrossValidatorModel_3600b24605f7"
     ]
    }
   ],
   "source": [
    "for depth in range(2, 30, 7):\n",
    "    rf_depth_time = []\n",
    "    for trees in range(2, 50, 8):\n",
    "        start = time.time()\n",
    "    \n",
    "        # Regressor\n",
    "        rfr = RandomForestRegressor()\n",
    "\n",
    "        # Parameter Grid\n",
    "        rf_paramGrid = ParamGridBuilder()\\\n",
    "          .addGrid(rfr.maxDepth, [depth])\\\n",
    "          .addGrid(rfr.numTrees, [trees])\\\n",
    "          .build()\n",
    "\n",
    "        # Cross-Validator\n",
    "        rf_crossval = CrossValidator(estimator=rfr,\n",
    "                                  estimatorParamMaps=rf_paramGrid,\n",
    "                                  evaluator=evaluator,\n",
    "                                  numFolds=10)\n",
    "\n",
    "        # Fit\n",
    "        rf_crossval.fit(train)\n",
    "    \n",
    "        rf_depth_time.append(time.time() - start)\n",
    "    rf_exec_time.append(rf_depth_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T22:11:05.976236Z",
     "start_time": "2019-03-12T22:11:05.956565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22.050609827041626, 20.224673748016357, 22.78614091873169, 24.931244611740112, 28.143370389938354, 29.912338256835938], [52.8497257232666, 97.263028383255, 115.99773979187012, 132.09366583824158, 143.721209526062, 154.19694328308105], [76.41726112365723, 146.3692603111267, 179.22828483581543, 206.7002022266388, 229.7097511291504, 253.48752355575562], [74.20605802536011, 145.59734225273132, 179.36284589767456, 209.45955324172974, 234.04102563858032, 257.59033036231995]]"
     ]
    }
   ],
   "source": [
    "print(rf_exec_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T22:11:05.997533Z",
     "start_time": "2019-03-12T22:11:05.977902Z"
    }
   },
   "outputs": [],
   "source": [
    "gbt_exec_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T22:55:00.745227Z",
     "start_time": "2019-03-12T22:11:05.999443Z"
    }
   },
   "outputs": [],
   "source": [
    "for depth in range(2, 11, 4):\n",
    "    gbt_depth_time = []\n",
    "    for iters in range(2, 11, 4):\n",
    "        start = time.time()\n",
    "    \n",
    "        # Regressor\n",
    "        gbt = GBTRegressor()\n",
    "\n",
    "        # Parameter Grid\n",
    "        gbt_paramGrid = ParamGridBuilder()\\\n",
    "          .addGrid(gbt.maxDepth, [depth])\\\n",
    "          .addGrid(gbt.maxIter, [iters])\\\n",
    "          .build()\n",
    "\n",
    "        # Cross-Validator\n",
    "        gbt_crossval = CrossValidator(estimator=gbt,\n",
    "                                  estimatorParamMaps=gbt_paramGrid,\n",
    "                                  evaluator=evaluator,\n",
    "                                  numFolds=10)\n",
    "\n",
    "        # Fit\n",
    "        gbt_model = gbt_crossval.fit(train)\n",
    "    \n",
    "        gbt_depth_time.append(time.time() - start)\n",
    "    gbt_exec_time.append(gbt_depth_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T22:55:00.782140Z",
     "start_time": "2019-03-12T22:55:00.747056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[28.97140598297119, 75.42643928527832, 123.2839126586914], [59.12826347351074, 169.164710521698, 290.4194505214691], [123.34894919395447, 383.85175943374634, 683.8586118221283]]"
     ]
    }
   ],
   "source": [
    "print(gbt_exec_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
