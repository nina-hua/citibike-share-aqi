{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Cluster Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mongo-Spark Connector Configuration to Connect to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T23:01:47.250958Z",
     "start_time": "2019-02-18T23:01:47.221540Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars.packages': 'org.mongodb.spark:mongo-spark-connector_2.11:2.4.0'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1550528955551_0033</td><td>pyspark</td><td>shutting_down</td><td><a target=\"_blank\" href=\"http://ip-172-31-37-174.us-west-2.compute.internal:20888/proxy/application_1550528955551_0033/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-43-43.us-west-2.compute.internal:8042/node/containerlogs/container_1550528955551_0033_01_000001/livy\">Link</a></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"conf\": {\"spark.jars.packages\": \"org.mongodb.spark:mongo-spark-connector_2.11:2.4.0\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T23:02:14.105347Z",
     "start_time": "2019-02-18T23:01:47.254649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1550528955551_0040</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-37-174.us-west-2.compute.internal:20888/proxy/application_1550528955551_0040/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-43-43.us-west-2.compute.internal:8042/node/containerlogs/container_1550528955551_0040_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, when, count, col, lag, avg, lit\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "from pyspark.storagelevel import StorageLevel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start timing for notebook execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T23:02:14.114925Z",
     "start_time": "2019-02-18T23:02:14.109775Z"
    }
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Data from MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bikeshare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T23:02:19.396329Z",
     "start_time": "2019-02-18T23:02:14.118631Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- starttime: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "mongodb_path = \"mongodb://172.31.38.7/msds697.bikeshare\"\n",
    "\n",
    "spark_bike = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"group25\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", mongodb_path)\\\n",
    "    .config(\"spark.driver.memory\", \"24g\")\\\n",
    "    .config(\"spark.yarn.appMasterEnv.PYSPARK_PYTHON\", \"python36\")\\\n",
    "    .config(\"spark.executorEnv.PYSPARK_PYTHON\", \"python36\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "bikeshare = spark_bike.read.format(\"com.mongodb.spark.sql.DefaultSource\").load().drop('_id')  # Load Data\n",
    "bikeshare = bikeshare.select('starttime').filter(\"starttime != 'starttime'\").cache()  # Keep only starttime column and filter out erroneous header line\n",
    "\n",
    "bikeshare.printSchema()\n",
    "# bikeshare.count()  # 47758652"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AQI Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T23:02:21.648882Z",
     "start_time": "2019-02-18T23:02:19.398168Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- aqi: integer (nullable = true)\n",
      " |-- siteID: integer (nullable = true)\n",
      " |-- yyyy/mm/dd: string (nullable = true)\n",
      "\n",
      "12859"
     ]
    }
   ],
   "source": [
    "mongodb_path = \"mongodb://172.31.38.7/aqi.aqi\"\n",
    "\n",
    "spark_aqi = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"group25\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", mongodb_path)\\\n",
    "    .config(\"spark.driver.memory\", \"24g\")\\\n",
    "    .config(\"spark.yarn.appMasterEnv.PYSPARK_PYTHON\", \"python36\")\\\n",
    "    .config(\"spark.executorEnv.PYSPARK_PYTHON\", \"python36\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "aqi = spark_aqi.read.format(\"com.mongodb.spark.sql.DefaultSource\").load().drop('_id').cache()\n",
    "aqi = aqi.select(['aqi', 'siteID', 'yyyy/mm/dd'])  # Keep only starttime column\n",
    "\n",
    "aqi.printSchema()\n",
    "aqi_total_count = aqi.count()\n",
    "print(aqi_total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T23:02:21.669414Z",
     "start_time": "2019-02-18T23:02:21.650725Z"
    }
   },
   "outputs": [],
   "source": [
    "def formatDateRide(mydate):\n",
    "    \"\"\"\n",
    "    Re-formats date to match bike date \n",
    "    \"\"\"\n",
    "    mydate = mydate.split(' ')[0]\n",
    "    try:\n",
    "        objDate = datetime.strptime(mydate, '%Y-%m-%d')\n",
    "        return datetime.strftime(objDate,'%-m/%-d/%Y')\n",
    "    except ValueError:\n",
    "        return mydate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T23:02:21.898886Z",
     "start_time": "2019-02-18T23:02:21.671060Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- count: long (nullable = false)"
     ]
    }
   ],
   "source": [
    "# Extract only date from timestamp\n",
    "date_udf = udf(lambda date: formatDateRide(date))\n",
    "daily_ridership = bikeshare.select(\"starttime\").withColumn(\"date\", date_udf(\"starttime\")).groupBy(\"date\").count()\n",
    "daily_ridership.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily AQI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T23:02:21.918168Z",
     "start_time": "2019-02-18T23:02:21.900692Z"
    }
   },
   "outputs": [],
   "source": [
    "def formatDate(mydate):\n",
    "    \"\"\"\n",
    "    Re-formats date to match bike date \n",
    "    \"\"\"\n",
    "    objDate = datetime.strptime(mydate, '%Y/%m/%d')\n",
    "    return datetime.strftime(objDate,'%-m/%-d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T23:02:22.143909Z",
     "start_time": "2019-02-18T23:02:21.919936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- max(AQI): integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "date_udf = udf(lambda date: formatDate(date))\n",
    "\n",
    "# Changing format of date column\n",
    "aqi = aqi.withColumn(\"date\", date_udf(\"yyyy/mm/dd\")).drop(\"siteID\", \"yyyy/mm/dd\")\n",
    "\n",
    "# Maintaining only the two stations near NYC\n",
    "NYC_sites = aqi.filter(\"SiteID == 360610135 or SiteID == 340171002\")\n",
    "\n",
    "# Group by Date\n",
    "daily_aqi = NYC_sites.groupBy(\"date\").max(\"AQI\")\n",
    "\n",
    "daily_aqi.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining AQI & Bike Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T23:02:22.372857Z",
     "start_time": "2019-02-18T23:02:22.145914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      " |-- max(AQI): integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "bike_aqi_joined = daily_ridership.join(daily_aqi, 'date','inner')\n",
    "bike_aqi_joined.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous Day's AQI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T23:02:22.598829Z",
     "start_time": "2019-02-18T23:02:22.374709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      " |-- max(AQI): integer (nullable = true)\n",
      " |-- pre_AQI: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "# Adding AQI lagged at 1 day\n",
    "bike_aqi_joined = bike_aqi_joined.select('date', 'count', 'max(AQI)',\\\n",
    "                       lag('max(AQI)', 1).over(Window.orderBy('date'))\\\n",
    "                      .alias('pre_AQI'))\n",
    "\n",
    "bike_aqi_joined.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Season Indicator Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T23:02:22.620045Z",
     "start_time": "2019-02-18T23:02:22.600851Z"
    }
   },
   "outputs": [],
   "source": [
    "def getMonth(inval):\n",
    "    try:\n",
    "        return int(datetime.strptime(inval, \"%m/%d/%Y\").month)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def getYear(inval):\n",
    "    try:\n",
    "        return int(datetime.strptime(inval, \"%m/%d/%Y\").year)\n",
    "    except ValueError:\n",
    "        return None\n",
    "def getSeason(month):\n",
    "    season_dict = {'winter' : [1, 2, 12],\n",
    "                   'summer' : [6, 7, 8],\n",
    "                   'spring' : [3, 4, 5],\n",
    "                   'autumn' : [9, 10, 11]}\n",
    "    for season in season_dict:\n",
    "        if month in season_dict.get(season):\n",
    "            return season "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T23:02:22.845669Z",
     "start_time": "2019-02-18T23:02:22.621905Z"
    }
   },
   "outputs": [],
   "source": [
    "year_udf = udf(lambda date: getYear(date))\n",
    "month_udf = udf(lambda date: getMonth(date))\n",
    "season_udf = udf(lambda month: getSeason(int(month)))\n",
    "\n",
    "bike_aqi_joined = bike_aqi_joined.withColumn(\"month\", month_udf(bike_aqi_joined.date))\\\n",
    "                .withColumn(\"year\", year_udf(bike_aqi_joined.date))\n",
    "bike_aqi_joined = bike_aqi_joined.withColumn(\"season\", season_udf(bike_aqi_joined.month))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Out All Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T23:02:23.073396Z",
     "start_time": "2019-02-18T23:02:22.847643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- count: long (nullable = false)\n",
      " |-- max(AQI): integer (nullable = true)\n",
      " |-- pre_AQI: integer (nullable = true)\n",
      " |-- season: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "bike_ml = bike_aqi_joined.filter(bike_aqi_joined.pre_AQI.isNotNull()).drop('date', 'month', 'year')\n",
    "bike_ml.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Categorical Values to One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T23:10:24.076867Z",
     "start_time": "2019-02-18T23:02:23.075267Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 44718)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 696, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 266, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 254, in authenticate_and_accum_updates\n",
      "    received_token = self.rfile.read(len(auth_token))\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "----------------------------------------"
     ]
    }
   ],
   "source": [
    "# Convert to Numeric type first\n",
    "# First time usually doesn't work as the data is fetched but somehow not computing properly\n",
    "# Second time's a charm. Execution is fast since the data is now actually loaded\n",
    "for i in range(2):\n",
    "    si = StringIndexer(inputCol=\"season\", outputCol=\"season-num\")\n",
    "    sm = si.fit(bike_ml)\n",
    "    bike_num = sm.transform(bike_ml).drop(\"season\")\n",
    "    bike_num = bike_num.withColumnRenamed(\"season-num\", \"season\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T23:10:24.300519Z",
     "start_time": "2019-02-18T23:10:24.078797Z"
    }
   },
   "outputs": [],
   "source": [
    "# Then One-Hot encode numerics\n",
    "onehotenc = OneHotEncoder(inputCol=\"season\", outputCol=\"season-onehot\", dropLast=True)\n",
    "bike_ohe = onehotenc.transform(bike_num).drop(\"season\")\n",
    "bike_ohe = bike_ohe.withColumnRenamed(\"season-onehot\", \"season\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T23:10:24.324453Z",
     "start_time": "2019-02-18T23:10:24.304512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- count: long (nullable = false)\n",
      " |-- max(AQI): integer (nullable = true)\n",
      " |-- pre_AQI: integer (nullable = true)\n",
      " |-- season: vector (nullable = true)"
     ]
    }
   ],
   "source": [
    "bike_ohe.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Feature Vector and Label Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T23:12:34.014775Z",
     "start_time": "2019-02-18T23:10:24.326328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(5,[0,1],[16009.0...|   40|\n",
      "|(5,[0,1],[5500.0,...|   28|\n",
      "|(5,[0,1],[14275.0...|   26|\n",
      "|(5,[0,1],[23232.0...|   46|\n",
      "|(5,[0,1],[28252.0...|   84|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "input_cols = [\"count\", \"pre_AQI\", \"season\"]\n",
    "va = VectorAssembler(outputCol=\"features\", inputCols=input_cols)\n",
    "bike_labeled = va.transform(bike_ohe).select(\"features\", \"max(AQI)\").withColumnRenamed(\"max(AQI)\", \"label\")\n",
    "\n",
    "bike_labeled.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T23:12:34.243590Z",
     "start_time": "2019-02-18T23:12:34.018511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[features: vector, label: int]"
     ]
    }
   ],
   "source": [
    "# Persist on Memory for Faster Access\n",
    "bike_labeled.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "80/20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T23:12:34.470228Z",
     "start_time": "2019-02-18T23:12:34.247245Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test = bike_labeled.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline modeling would be to predict the AQI for the day to be to be the mean AQI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T23:14:44.184047Z",
     "start_time": "2019-02-18T23:12:34.474619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average AQI Over Entire Dataset: 27.19016083254494"
     ]
    }
   ],
   "source": [
    "avg_aqi = bike_ml.select('max(AQI)').agg({\"max(AQI)\": \"avg\"}).collect()[0][0]  # best constant prediction\n",
    "print(f\"Average AQI Over Entire Dataset: {avg_aqi}\")  # best constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T23:16:53.864604Z",
     "start_time": "2019-02-18T23:14:44.185912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RMSE = 11.7702"
     ]
    }
   ],
   "source": [
    "baseline = test\n",
    "baseline = baseline.withColumn('prediction', lit(avg_aqi))\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "print(f'Baseline RMSE = {round(evaluator.evaluate(baseline),4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T23:17:36.972710Z",
     "start_time": "2019-02-18T23:16:53.866385Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build Pipeline\n",
    "scaler = StandardScaler(inputCol='features', outputCol='scaled_feats', withStd=True, withMean=True)  # Feature Scaling\n",
    "lr = LinearRegression(featuresCol='scaled_feats')  # Linear Regressor\n",
    "pipeline = Pipeline(stages=[scaler, lr])  # Pipeline\n",
    "\n",
    "# Parameter Grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.elasticNetParam, np.arange(0, 1, 0.1)) \\\n",
    "    .build()\n",
    "\n",
    "# Cross-Validator\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(),\n",
    "                          numFolds=10)\n",
    "\n",
    "# Fit\n",
    "lr_model = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T23:17:37.199538Z",
     "start_time": "2019-02-18T23:17:36.974614Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 9.9255\n",
      "Feature Coefficients:\n",
      "\tcount: 2.86483461570471\n",
      "\tpre_AQI: -1.1401991446520938\n",
      "\tseason2: -1.9769305386562357\n",
      "\tseason1: -6.463782745203672\n",
      "\tseason3: -7.965938192983097"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "pred = lr_model.transform(test)\n",
    "\n",
    "# Performance Evaluation\n",
    "print(f'RMSE = {round(evaluator.evaluate(pred),4)}')\n",
    "\n",
    "# Coefficients\n",
    "print(\"Feature Coefficients:\")\n",
    "features = ['count', 'pre_AQI', 'season1', 'season2', 'season3']\n",
    "feat_coef = sorted(zip(features, lr_model.bestModel.stages[-1].coefficients), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for feat in feat_coef:\n",
    "    print(f\"\\t{feat[0]}: {feat[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T23:21:44.805810Z",
     "start_time": "2019-02-18T23:17:37.201175Z"
    }
   },
   "outputs": [],
   "source": [
    "# Regressor\n",
    "rfr = RandomForestRegressor()\n",
    "\n",
    "# Parameter Grid\n",
    "rf_paramGrid = ParamGridBuilder()\\\n",
    "  .addGrid(rfr.maxDepth, list(range(2, 30, 7)))\\\n",
    "  .addGrid(rfr.numTrees, list(range(2, 50, 8)))\\\n",
    "  .build()\n",
    "\n",
    "# Cross-Validator\n",
    "rf_crossval = CrossValidator(estimator=rfr,\n",
    "                          estimatorParamMaps=rf_paramGrid,\n",
    "                          evaluator=RegressionEvaluator(),\n",
    "                          numFolds=10)\n",
    "\n",
    "# Fit\n",
    "rfr_model = rf_crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T23:21:45.032892Z",
     "start_time": "2019-02-18T23:21:44.807680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 9.9357\n",
      "Feature Importance:\n",
      "\tcount: 0.2865708722578546\n",
      "\tseason3: 0.27572933037627734\n",
      "\tpre_AQI: 0.24108170951205712\n",
      "\tseason1: 0.11320314081362665\n",
      "\tseason2: 0.0834149470401842"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "pred = rfr_model.transform(test)\n",
    "\n",
    "# Performance Evaluation\n",
    "print(f'RMSE = {round(evaluator.evaluate(pred),4)}')\n",
    "\n",
    "# Feature Importance\n",
    "print(\"Feature Importance:\")\n",
    "feat_impor = sorted([(feat, rfr_model.bestModel.featureImportances[i]) for i,feat in enumerate(features)],\n",
    "                    key=lambda x: x[1], reverse=True)\n",
    "for feat in feat_impor:\n",
    "    print(f\"\\t{feat[0]}: {feat[1]}\")\n",
    "\n",
    "# Decision Tree if desired but too big to print\n",
    "# print(rfr_model._call_java('toDebugString'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T23:33:44.742174Z",
     "start_time": "2019-02-18T23:21:45.034924Z"
    }
   },
   "outputs": [],
   "source": [
    "# Regressor\n",
    "gbt = GBTRegressor()\n",
    "\n",
    "# Parameter Grid\n",
    "gbt_paramGrid = ParamGridBuilder()\\\n",
    "  .addGrid(gbt.maxDepth, list(range(2, 30, 7)))\\\n",
    "  .addGrid(gbt.maxIter, list(range(2, 18, 4)))\\\n",
    "  .build()\n",
    "\n",
    "# Cross-Validator\n",
    "gbt_crossval = CrossValidator(estimator=gbt,\n",
    "                          estimatorParamMaps=gbt_paramGrid,\n",
    "                          evaluator=RegressionEvaluator(),\n",
    "                          numFolds=10)\n",
    "\n",
    "# Fit\n",
    "gbt_model = gbt_crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T23:33:44.972826Z",
     "start_time": "2019-02-18T23:33:44.744215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 10.0666\n",
      "Feature Importance:\n",
      "\tpre_AQI: 0.3359657682344603\n",
      "\tcount: 0.3160366517992946\n",
      "\tseason2: 0.25298448498873854\n",
      "\tseason3: 0.06929777629147833\n",
      "\tseason1: 0.025715318686028332"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "pred = gbt_model.transform(test)\n",
    "\n",
    "# Performance Evaluation\n",
    "print(f'RMSE = {round(evaluator.evaluate(pred),4)}')\n",
    "\n",
    "# Feature Importance\n",
    "print(\"Feature Importance:\")\n",
    "feat_impor = sorted([(feat, gbt_model.bestModel.featureImportances[i]) for i,feat in enumerate(features)],\n",
    "                    key=lambda x: x[1], reverse=True)\n",
    "for feat in feat_impor:\n",
    "    print(f\"\\t{feat[0]}: {feat[1]}\")\n",
    "\n",
    "# Decision Tree if desired but too big to print\n",
    "# print(gb_model._call_java('toDebugString'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Execution Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T23:33:44.978390Z",
     "start_time": "2019-02-18T23:33:44.974771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Execution Time: 31.514418995380403 minutes\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "print(f\"Total Execution Time: {(time.time() - start) / 60} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Final Dataframe to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T23:33:45.006685Z",
     "start_time": "2019-02-18T23:33:44.980318Z"
    }
   },
   "outputs": [],
   "source": [
    "# bike_aqi_joined.write.csv('s3://msds697-phil/BikeshareDF', 'overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
